{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1:Implementing a Basic Autoencoder"
      ],
      "metadata": {
        "id": "JaR3vqytZzWn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.datasets import mnist"
      ],
      "metadata": {
        "id": "70wMRYC_Zyix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MNIST dataset\n",
        "(x_train, _), (x_test, _) = mnist.load_data()\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "x_train = x_train.reshape((len(x_train), 784))\n",
        "x_test = x_test.reshape((len(x_test), 784))"
      ],
      "metadata": {
        "id": "S5CT7EvhZ8gy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the autoencoder model\n",
        "latent_dim = 32  # You can modify this to 16 or 64 to analyze effects\n",
        "\n",
        "# Encoder\n",
        "input_img = Input(shape=(784,))\n",
        "encoded = Dense(latent_dim, activation='relu')(input_img)\n",
        "\n",
        "# Decoder\n",
        "decoded = Dense(784, activation='sigmoid')(encoded)\n",
        "\n",
        "# Autoencoder model\n",
        "autoencoder = Model(input_img, decoded)\n",
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "# Train the model\n",
        "autoencoder.fit(x_train, x_train, epochs=20, batch_size=256, shuffle=True, validation_data=(x_test, x_test))\n",
        "\n",
        "# Get reconstructed images\n",
        "reconstructed_imgs = autoencoder.predict(x_test)\n"
      ],
      "metadata": {
        "id": "NVoknIZEZ-jx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot original vs reconstructed images\n",
        "def plot_images(original, reconstructed, n=10):\n",
        "    plt.figure(figsize=(20, 4))\n",
        "    for i in range(n):\n",
        "        # Original images\n",
        "        ax = plt.subplot(2, n, i + 1)\n",
        "        plt.imshow(original[i].reshape(28, 28), cmap='gray')\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Reconstructed images\n",
        "        ax = plt.subplot(2, n, i + 1 + n)\n",
        "        plt.imshow(reconstructed[i].reshape(28, 28), cmap='gray')\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "plot_images(x_test, reconstructed_imgs)\n"
      ],
      "metadata": {
        "id": "VNzkoJfmaD6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: Implementing a Denoising Autoencoder"
      ],
      "metadata": {
        "id": "030zkGvWaQvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.datasets import mnist"
      ],
      "metadata": {
        "id": "D6D8zTzVaWRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MNIST dataset\n",
        "(x_train, _), (x_test, _) = mnist.load_data()\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "x_train = x_train.reshape((len(x_train), 784))\n",
        "x_test = x_test.reshape((len(x_test), 784))"
      ],
      "metadata": {
        "id": "0AeGYR3iaaLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add Gaussian noise to input images\n",
        "def add_noise(images, noise_factor=0.5):\n",
        "    noisy_images = images + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=images.shape)\n",
        "    noisy_images = np.clip(noisy_images, 0., 1.)\n",
        "    return noisy_images\n",
        "\n",
        "x_train_noisy = add_noise(x_train)\n",
        "x_test_noisy = add_noise(x_test)"
      ],
      "metadata": {
        "id": "kxIqCV2Xacas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the denoising autoencoder model\n",
        "latent_dim = 32  # Modify for analysis\n",
        "\n",
        "# Encoder\n",
        "input_img = Input(shape=(784,))\n",
        "encoded = Dense(latent_dim, activation='relu')(input_img)\n",
        "\n",
        "# Decoder\n",
        "decoded = Dense(784, activation='sigmoid')(encoded)\n",
        "\n",
        "# Autoencoder model\n",
        "autoencoder = Model(input_img, decoded)\n",
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "# Train the model\n",
        "autoencoder.fit(x_train_noisy, x_train, epochs=20, batch_size=256, shuffle=True, validation_data=(x_test_noisy, x_test))\n",
        "\n",
        "# Get reconstructed images\n",
        "reconstructed_imgs = autoencoder.predict(x_test_noisy)"
      ],
      "metadata": {
        "id": "jM86bvvAaffl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: Implementing an RNN for Text Generation"
      ],
      "metadata": {
        "id": "VF7IVqITajRx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Input\n",
        "import random\n",
        "import sys\n",
        "import re"
      ],
      "metadata": {
        "id": "9PRmQeecansS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset (Example: Shakespeare Sonnets)\n",
        "path = tf.keras.utils.get_file(\"shakespeare.txt\", \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\")\n",
        "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read().lower()\n"
      ],
      "metadata": {
        "id": "n7f296seaqd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare character mapping\n",
        "chars = sorted(list(set(text)))\n",
        "char_to_idx = {c: i for i, c in enumerate(chars)}\n",
        "idx_to_char = {i: c for i, c in enumerate(chars)}\n",
        "\n",
        "# Create sequences\n",
        "seq_length = 40\n",
        "step = 3\n",
        "sentences = []\n",
        "next_chars = []\n",
        "for i in range(0, len(text) - seq_length, step):\n",
        "    sentences.append(text[i: i + seq_length])\n",
        "    next_chars.append(text[i + seq_length])\n",
        "\n",
        "X = np.zeros((len(sentences), seq_length, len(chars)), dtype=np.bool_)\n",
        "y = np.zeros((len(sentences), len(chars)), dtype=np.bool_)\n",
        "\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "        X[i, t, char_to_idx[char]] = 1\n",
        "    y[i, char_to_idx[next_chars[i]]] = 1\n"
      ],
      "metadata": {
        "id": "SLibvib6auzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define RNN model\n",
        "model = Sequential([\n",
        "    Input(shape=(seq_length, len(chars))),\n",
        "    LSTM(128),\n",
        "    Dense(len(chars), activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "# Train model\n",
        "model.fit(X, y, batch_size=128, epochs=10)"
      ],
      "metadata": {
        "id": "t_Ca5Lf-ayqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4 :Sentiment Classification Using RNN"
      ],
      "metadata": {
        "id": "eETDMA05aFLx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "A6jinxFdYpSp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Load and preprocess the IMDB dataset"
      ],
      "metadata": {
        "id": "ltZHulyFZDIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "max_features = 10000  # Number of words to consider as features\n",
        "maxlen = 200  # Cut off reviews after this many words\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "# Pad sequences to ensure uniform input length\n",
        "X_train = pad_sequences(X_train, maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, maxlen=maxlen)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EgrGNL5ZADk",
        "outputId": "05b027b1-afa6-4521-c14f-e076a259bce2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Build the LSTM model"
      ],
      "metadata": {
        "id": "sfjnV8zqZFBB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Embedding(max_features, 128),  # Removed input_length parameter\n",
        "    LSTM(64, return_sequences=False),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "5mauyJGuZHxk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Train the model"
      ],
      "metadata": {
        "id": "bbenXkX_ZLCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train,\n",
        "                   epochs=5,\n",
        "                   batch_size=32,\n",
        "                   validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyPEel-xZM7O",
        "outputId": "d6bb8821-bda0-4ffc-db68-dfc581b6ec38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 175ms/step - accuracy: 0.7410 - loss: 0.4984 - val_accuracy: 0.8694 - val_loss: 0.3124\n",
            "Epoch 2/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 173ms/step - accuracy: 0.8784 - loss: 0.2920 - val_accuracy: 0.8628 - val_loss: 0.3309\n",
            "Epoch 3/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 154ms/step - accuracy: 0.9289 - loss: 0.1904 - val_accuracy: 0.8690 - val_loss: 0.3512\n",
            "Epoch 4/5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Evaluate the model"
      ],
      "metadata": {
        "id": "TYscwuOfZQYV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
        "\n",
        "# Generate confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Generate classification report\n",
        "report = classification_report(y_test, y_pred, target_names=['Negative', 'Positive'])"
      ],
      "metadata": {
        "id": "BzPoZ-aLZRK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Visualize results"
      ],
      "metadata": {
        "id": "TL8QZod1ZTCp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(report)"
      ],
      "metadata": {
        "id": "v_CQu8kDZVR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training history\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YBXecF2MZYNY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}